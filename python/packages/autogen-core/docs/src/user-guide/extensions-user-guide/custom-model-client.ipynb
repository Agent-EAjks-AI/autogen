{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Model Clients\n",
    "\n",
    "This guide shows how to create custom model clients by subclassing the {py:class}`~autogen_core.models.ChatCompletionClient` base class. Custom model clients allow you to integrate any LLM or AI service that isn't directly supported by AutoGen's built-in model clients.\n",
    "\n",
    "In this tutorial, we'll create a custom model client using HuggingFace Transformers as the underlying model API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the ChatCompletionClient Interface\n",
    "\n",
    "All model clients in AutoGen implement the {py:class}`~autogen_core.models.ChatCompletionClient` protocol. This abstract base class defines two key methods that you must implement:\n",
    "\n",
    "- `create()`: Creates a single response from the model\n",
    "- `create_stream()`: Creates a stream of string chunks from the model ending with a CreateResult\n",
    "\n",
    "Let's examine the interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_core.models import ChatCompletionClient\n",
    "import inspect\n",
    "\n",
    "# View the abstract methods that need to be implemented\n",
    "abstract_methods = [method for method in dir(ChatCompletionClient) \n",
    "                   if getattr(getattr(ChatCompletionClient, method, None), '__isabstractmethod__', False)]\n",
    "print(\"Abstract methods to implement:\")\n",
    "for method in abstract_methods:\n",
    "    print(f\"- {method}\")\n",
    "\n",
    "# Show the signature of the create method\n",
    "print(\"\\nCreate method signature:\")\n",
    "print(inspect.signature(ChatCompletionClient.create))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Imports and Setup\n",
    "\n",
    "Let's import all the necessary modules for creating our custom model client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import warnings\n",
    "from typing import AsyncGenerator, List, Literal, Mapping, Optional, Sequence, Union, Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from transformers import pipeline, Pipeline\n",
    "import torch\n",
    "\n",
    "from autogen_core import CancellationToken, Component\n",
    "from autogen_core.models import (\n",
    "    ChatCompletionClient,\n",
    "    CreateResult,\n",
    "    LLMMessage,\n",
    "    RequestUsage,\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    AssistantMessage,\n",
    "    ModelCapabilities,\n",
    "    ModelInfo,\n",
    ")\n",
    "from autogen_core.tools import Tool, ToolSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a HuggingFace Custom Model Client\n",
    "\n",
    "Now let's implement our custom model client. This example uses HuggingFace's text generation pipeline to create a simple chat completion client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceConfig(BaseModel):\n",
    "    \"\"\"Configuration for HuggingFace model client.\"\"\"\n",
    "    model_name: str = \"microsoft/DialoGPT-small\"\n",
    "    max_new_tokens: int = 100\n",
    "    temperature: float = 0.7\n",
    "    do_sample: bool = True\n",
    "    device: Optional[str] = None  # Let HF auto-detect if None\n",
    "\n",
    "\n",
    "class HuggingFaceChatCompletionClient(\n",
    "    ChatCompletionClient, Component[HuggingFaceConfig]\n",
    "):\n",
    "    \"\"\"A custom model client that uses HuggingFace transformers.\n",
    "    \n",
    "    This example demonstrates how to create a custom model client by:\n",
    "    1. Subclassing ChatCompletionClient\n",
    "    2. Implementing the required abstract methods\n",
    "    3. Handling message conversion and response formatting\n",
    "    \"\"\"\n",
    "    \n",
    "    component_type = \"HuggingFaceChatCompletionClient\"\n",
    "    component_config_schema = HuggingFaceConfig\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        # Initialize the Component base class\n",
    "        super().__init__()\n",
    "        \n",
    "        # Parse configuration\n",
    "        config = HuggingFaceConfig(**kwargs)\n",
    "        \n",
    "        # Initialize the HuggingFace pipeline\n",
    "        self._pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=config.model_name,\n",
    "            device=0 if torch.cuda.is_available() and config.device != \"cpu\" else -1,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        )\n",
    "        \n",
    "        # Store configuration\n",
    "        self._config = config\n",
    "        \n",
    "        # Initialize usage tracking\n",
    "        self._total_usage = RequestUsage(prompt_tokens=0, completion_tokens=0)\n",
    "        \n",
    "        # Model capabilities - adjust based on your model's capabilities\n",
    "        self._model_info = ModelInfo(\n",
    "            vision=False,\n",
    "            function_calling=False,\n",
    "            json_output=False,\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def capabilities(self) -> ModelCapabilities:\n",
    "        \"\"\"Return the capabilities of this model.\"\"\"\n",
    "        return self._model_info\n",
    "    \n",
    "    @property\n",
    "    def model_info(self) -> ModelInfo:\n",
    "        \"\"\"Return the model information.\"\"\"\n",
    "        return self._model_info\n",
    "    \n",
    "    @property\n",
    "    def total_usage(self) -> RequestUsage:\n",
    "        \"\"\"Return the total token usage for this client.\"\"\"\n",
    "        return self._total_usage\n",
    "    \n",
    "    @property\n",
    "    def actual_usage(self) -> RequestUsage:\n",
    "        \"\"\"Return the actual usage (same as total for this implementation).\"\"\"\n",
    "        return self._total_usage\n",
    "    \n",
    "    async def close(self) -> None:\n",
    "        \"\"\"Close the client and clean up resources.\"\"\"\n",
    "        # For this implementation, we don't need to close anything\n",
    "        # but in a real implementation you might need to close connections, etc.\n",
    "        pass\n",
    "    \n",
    "    def count_tokens(self, messages: Sequence[LLMMessage], *, tools: Sequence[Tool | ToolSchema] = []) -> int:\n",
    "        \"\"\"Count the number of tokens in the messages.\"\"\"\n",
    "        prompt = self._messages_to_prompt(messages)\n",
    "        return self._estimate_tokens(prompt)\n",
    "    \n",
    "    def remaining_tokens(self, messages: Sequence[LLMMessage], *, tools: Sequence[Tool | ToolSchema] = []) -> int:\n",
    "        \"\"\"Return the number of remaining tokens for the context window.\n",
    "        \n",
    "        For this simple implementation, we'll assume a context window of 2048 tokens.\n",
    "        In a real implementation, you'd use the actual model's context window size.\n",
    "        \"\"\"\n",
    "        used_tokens = self.count_tokens(messages, tools=tools)\n",
    "        context_window = 2048  # Assumed context window size\n",
    "        return max(0, context_window - used_tokens)\n",
    "    \n",
    "    def _messages_to_prompt(self, messages: Sequence[LLMMessage]) -> str:\n",
    "        \"\"\"Convert a sequence of messages to a prompt string.\n",
    "        \n",
    "        This is a simple implementation that concatenates messages.\n",
    "        For production use, you might want to use the model's specific\n",
    "        chat template if available.\n",
    "        \"\"\"\n",
    "        prompt_parts = []\n",
    "        \n",
    "        for message in messages:\n",
    "            if isinstance(message, SystemMessage):\n",
    "                prompt_parts.append(f\"System: {message.content}\")\n",
    "            elif isinstance(message, UserMessage):\n",
    "                # Handle both string and list content\n",
    "                if isinstance(message.content, str):\n",
    "                    prompt_parts.append(f\"User: {message.content}\")\n",
    "                else:\n",
    "                    # For complex content (images, etc.), extract text only\n",
    "                    text_content = \"\"\n",
    "                    for item in message.content:\n",
    "                        if hasattr(item, 'text'):\n",
    "                            text_content += item.text\n",
    "                    prompt_parts.append(f\"User: {text_content}\")\n",
    "            elif isinstance(message, AssistantMessage):\n",
    "                if isinstance(message.content, str):\n",
    "                    prompt_parts.append(f\"Assistant: {message.content}\")\n",
    "                # Note: This simple implementation doesn't handle function calls\n",
    "        \n",
    "        prompt = \"\\n\".join(prompt_parts)\n",
    "        prompt += \"\\nAssistant:\"  # Prompt for the next assistant response\n",
    "        return prompt\n",
    "    \n",
    "    def _estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Simple token estimation. For production, use proper tokenization.\"\"\"\n",
    "        # Very rough estimation: assume ~4 characters per token\n",
    "        return len(text) // 4\n",
    "    \n",
    "    async def create(\n",
    "        self,\n",
    "        messages: Sequence[LLMMessage],\n",
    "        *,\n",
    "        tools: Sequence[Tool | ToolSchema] = [],\n",
    "        tool_choice: Tool | Literal[\"auto\", \"required\", \"none\"] = \"auto\",\n",
    "        json_output: Optional[bool | type[BaseModel]] = None,\n",
    "        extra_create_args: Mapping[str, Any] = {},\n",
    "        cancellation_token: Optional[CancellationToken] = None,\n",
    "    ) -> CreateResult:\n",
    "        \"\"\"Create a single response from the model.\"\"\"\n",
    "        \n",
    "        # Check for unsupported features\n",
    "        if tools:\n",
    "            raise ValueError(\"Tool calling is not supported by this model client\")\n",
    "        if json_output:\n",
    "            raise ValueError(\"JSON output is not supported by this model client\")\n",
    "        \n",
    "        # Convert messages to prompt\n",
    "        prompt = self._messages_to_prompt(messages)\n",
    "        \n",
    "        # Estimate input tokens\n",
    "        prompt_tokens = self._estimate_tokens(prompt)\n",
    "        \n",
    "        # Prepare generation arguments\n",
    "        generation_args = {\n",
    "            \"max_new_tokens\": self._config.max_new_tokens,\n",
    "            \"temperature\": self._config.temperature,\n",
    "            \"do_sample\": self._config.do_sample,\n",
    "            \"return_full_text\": False,  # Only return the generated part\n",
    "            \"pad_token_id\": self._pipeline.tokenizer.eos_token_id,\n",
    "        }\n",
    "        generation_args.update(extra_create_args)\n",
    "        \n",
    "        # Generate response using asyncio to avoid blocking\n",
    "        def _generate():\n",
    "            return self._pipeline(prompt, **generation_args)\n",
    "        \n",
    "        # Run in thread pool to avoid blocking the event loop\n",
    "        loop = asyncio.get_event_loop()\n",
    "        result = await loop.run_in_executor(None, _generate)\n",
    "        \n",
    "        # Extract the generated text\n",
    "        if isinstance(result, list) and len(result) > 0:\n",
    "            generated_text = result[0][\"generated_text\"].strip()\n",
    "        else:\n",
    "            generated_text = \"\"\n",
    "        \n",
    "        # Estimate completion tokens\n",
    "        completion_tokens = self._estimate_tokens(generated_text)\n",
    "        \n",
    "        # Create usage info\n",
    "        usage = RequestUsage(\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            completion_tokens=completion_tokens,\n",
    "        )\n",
    "        \n",
    "        # Update total usage\n",
    "        self._total_usage = RequestUsage(\n",
    "            prompt_tokens=self._total_usage.prompt_tokens + usage.prompt_tokens,\n",
    "            completion_tokens=self._total_usage.completion_tokens + usage.completion_tokens,\n",
    "        )\n",
    "        \n",
    "        # Return the result\n",
    "        return CreateResult(\n",
    "            finish_reason=\"stop\",\n",
    "            content=generated_text,\n",
    "            usage=usage,\n",
    "            cached=False,\n",
    "        )\n",
    "    \n",
    "    async def create_stream(\n",
    "        self,\n",
    "        messages: Sequence[LLMMessage],\n",
    "        *,\n",
    "        tools: Sequence[Tool | ToolSchema] = [],\n",
    "        tool_choice: Tool | Literal[\"auto\", \"required\", \"none\"] = \"auto\",\n",
    "        json_output: Optional[bool | type[BaseModel]] = None,\n",
    "        extra_create_args: Mapping[str, Any] = {},\n",
    "        cancellation_token: Optional[CancellationToken] = None,\n",
    "    ) -> AsyncGenerator[Union[str, CreateResult], None]:\n",
    "        \"\"\"Create a stream of string chunks from the model.\n",
    "        \n",
    "        Note: This is a simplified implementation that simulates streaming\n",
    "        by generating the full response and then yielding it in chunks.\n",
    "        For true streaming, you would need to use a streaming-capable model\n",
    "        or API.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate the full response first\n",
    "        result = await self.create(\n",
    "            messages,\n",
    "            tools=tools,\n",
    "            tool_choice=tool_choice,\n",
    "            json_output=json_output,\n",
    "            extra_create_args=extra_create_args,\n",
    "            cancellation_token=cancellation_token,\n",
    "        )\n",
    "        \n",
    "        # Simulate streaming by yielding chunks\n",
    "        if isinstance(result.content, str):\n",
    "            # Split into words and yield each word as a chunk\n",
    "            words = result.content.split()\n",
    "            for i, word in enumerate(words):\n",
    "                chunk = word + (\" \" if i < len(words) - 1 else \"\")\n",
    "                yield chunk\n",
    "                # Small delay to simulate streaming\n",
    "                await asyncio.sleep(0.01)\n",
    "        \n",
    "        # Yield the final result\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Custom Model Client\n",
    "\n",
    "Now let's test our custom model client. We'll create an instance and use it to generate responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of our custom model client\n",
    "# Note: This will download the model on first use\n",
    "model_client = HuggingFaceChatCompletionClient(\n",
    "    model_name=\"microsoft/DialoGPT-small\",  # Small model for quick testing\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Custom model client created successfully!\")\n",
    "print(f\"Model capabilities: {model_client.capabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Chat Completion\n",
    "\n",
    "Let's test the basic `create` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some test messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\", source=\"system\"),\n",
    "    UserMessage(content=\"Hello! How are you today?\", source=\"user\")\n",
    "]\n",
    "\n",
    "# Generate a response\n",
    "response = await model_client.create(messages)\n",
    "\n",
    "print(f\"Response: {response.content}\")\n",
    "print(f\"Finish reason: {response.finish_reason}\")\n",
    "print(f\"Usage: {response.usage}\")\n",
    "print(f\"Total usage so far: {model_client.total_usage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Response\n",
    "\n",
    "Now let's test the streaming functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test streaming with a different conversation\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a creative storyteller.\", source=\"system\"),\n",
    "    UserMessage(content=\"Tell me a short story about a robot.\", source=\"user\")\n",
    "]\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "print(\"---\")\n",
    "\n",
    "final_result = None\n",
    "async for chunk in model_client.create_stream(messages):\n",
    "    if isinstance(chunk, str):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    else:\n",
    "        # This is the final CreateResult\n",
    "        final_result = chunk\n",
    "\n",
    "print(\"\\n---\")\n",
    "print(f\"\\nFinal result usage: {final_result.usage if final_result else 'N/A'}\")\n",
    "print(f\"Total usage: {model_client.total_usage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using with AutoGen Agents\n",
    "\n",
    "Your custom model client can be used with AutoGen agents just like any built-in client. Here's a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_core import RoutedAgent, MessageContext, message_handler\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ChatMessage:\n",
    "    content: str\n",
    "\n",
    "class CustomModelAgent(RoutedAgent):\n",
    "    \"\"\"A simple agent that uses our custom model client.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_client: HuggingFaceChatCompletionClient):\n",
    "        super().__init__(\"Custom agent using HuggingFace model\")\n",
    "        self._model_client = model_client\n",
    "        self._system_message = SystemMessage(\n",
    "            content=\"You are a helpful AI assistant powered by a custom HuggingFace model.\",\n",
    "            source=\"system\"\n",
    "        )\n",
    "    \n",
    "    @message_handler\n",
    "    async def handle_chat_message(self, message: ChatMessage, ctx: MessageContext) -> str:\n",
    "        # Create messages for the model\n",
    "        messages = [\n",
    "            self._system_message,\n",
    "            UserMessage(content=message.content, source=\"user\")\n",
    "        ]\n",
    "        \n",
    "        # Generate response using our custom model client\n",
    "        response = await self._model_client.create(messages)\n",
    "        \n",
    "        return response.content\n",
    "\n",
    "# Create an agent with our custom model client\n",
    "agent = CustomModelAgent(model_client)\n",
    "print(\"Custom agent created successfully!\")\n",
    "print(f\"Agent created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Considerations\n",
    "\n",
    "When creating custom model clients, consider the following:\n",
    "\n",
    "### 1. Error Handling\n",
    "Implement proper error handling for network issues, model failures, and invalid inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of adding error handling to our client\n",
    "class RobustHuggingFaceChatCompletionClient(HuggingFaceChatCompletionClient):\n",
    "    \"\"\"Enhanced version with better error handling.\"\"\"\n",
    "    \n",
    "    async def create(\n",
    "        self,\n",
    "        messages: Sequence[LLMMessage],\n",
    "        **kwargs\n",
    "    ) -> CreateResult:\n",
    "        try:\n",
    "            return await super().create(messages, **kwargs)\n",
    "        except Exception as e:\n",
    "            # Log the error and provide a graceful fallback\n",
    "            print(f\"Error in model generation: {e}\")\n",
    "            return CreateResult(\n",
    "                finish_reason=\"error\",\n",
    "                content=\"I apologize, but I encountered an error while processing your request.\",\n",
    "                usage=RequestUsage(prompt_tokens=0, completion_tokens=0),\n",
    "                cached=False,\n",
    "            )\n",
    "\n",
    "print(\"Enhanced client with error handling defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuration Management\n",
    "Use Pydantic models for configuration to ensure type safety and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field, validator\n",
    "\n",
    "class AdvancedHuggingFaceConfig(BaseModel):\n",
    "    \"\"\"More comprehensive configuration with validation.\"\"\"\n",
    "    model_name: str = Field(description=\"HuggingFace model name or path\")\n",
    "    max_new_tokens: int = Field(default=100, ge=1, le=2048, description=\"Maximum tokens to generate\")\n",
    "    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description=\"Sampling temperature\")\n",
    "    top_p: Optional[float] = Field(default=None, ge=0.0, le=1.0, description=\"Nucleus sampling parameter\")\n",
    "    device: Optional[str] = Field(default=None, description=\"Device to run the model on\")\n",
    "    \n",
    "    @validator('model_name')\n",
    "    def validate_model_name(cls, v):\n",
    "        if not v.strip():\n",
    "            raise ValueError(\"Model name cannot be empty\")\n",
    "        return v\n",
    "\n",
    "# Example usage\n",
    "config = AdvancedHuggingFaceConfig(\n",
    "    model_name=\"microsoft/DialoGPT-small\",\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.8\n",
    ")\n",
    "print(f\"Configuration: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Token Usage Tracking\n",
    "Implement accurate token counting for cost tracking and rate limiting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of more accurate token counting\n",
    "def accurate_token_count(text: str, tokenizer) -> int:\n",
    "    \"\"\"More accurate token counting using the model's actual tokenizer.\"\"\"\n",
    "    if hasattr(tokenizer, 'encode'):\n",
    "        return len(tokenizer.encode(text))\n",
    "    else:\n",
    "        # Fallback to rough estimation\n",
    "        return len(text) // 4\n",
    "\n",
    "print(\"Token counting utility defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Capabilities Declaration\n",
    "Accurately declare your model's capabilities to ensure proper integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of different capability configurations\n",
    "def get_model_capabilities(model_name: str) -> ModelInfo:\n",
    "    \"\"\"Return capabilities based on the specific model.\"\"\"\n",
    "    \n",
    "    # This is a simplified example - in practice, you'd have a more\n",
    "    # comprehensive mapping of model capabilities\n",
    "    if \"gpt\" in model_name.lower():\n",
    "        return ModelInfo(\n",
    "            vision=False,\n",
    "            function_calling=True,  # Many GPT models support function calling\n",
    "            json_output=True,\n",
    "        )\n",
    "    elif \"vision\" in model_name.lower():\n",
    "        return ModelInfo(\n",
    "            vision=True,\n",
    "            function_calling=False,\n",
    "            json_output=False,\n",
    "        )\n",
    "    else:\n",
    "        # Conservative defaults\n",
    "        return ModelInfo(\n",
    "            vision=False,\n",
    "            function_calling=False,\n",
    "            json_output=False,\n",
    "        )\n",
    "\n",
    "print(\"Model capability detection utility defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "\n",
    "### Supporting Function Calling\n",
    "If your model supports function calling, implement tool handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for function calling implementation\n",
    "# This would require significant additional code to handle tools properly\n",
    "def handle_tools(tools: Sequence[Tool | ToolSchema], response_text: str):\n",
    "    \"\"\"Extract and handle function calls from the model response.\n",
    "    \n",
    "    This is a placeholder - actual implementation would depend on\n",
    "    how your model formats function calls in its output.\n",
    "    \"\"\"\n",
    "    # Parse the response for function calls\n",
    "    # Convert to FunctionCall objects\n",
    "    # Return appropriate content type\n",
    "    pass\n",
    "\n",
    "print(\"Function calling placeholder defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting JSON Output\n",
    "For structured output, implement JSON mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def handle_json_output(json_output: bool | type[BaseModel], prompt: str) -> str:\n",
    "    \"\"\"Modify the prompt to encourage JSON output.\n",
    "    \n",
    "    This is a simple approach - more sophisticated implementations\n",
    "    might use guided generation or constrained decoding.\n",
    "    \"\"\"\n",
    "    if json_output is True:\n",
    "        return prompt + \"\\n\\nPlease respond with valid JSON only.\"\n",
    "    elif isinstance(json_output, type) and issubclass(json_output, BaseModel):\n",
    "        schema = json_output.model_json_schema()\n",
    "        return f\"{prompt}\\n\\nPlease respond with JSON that matches this schema: {json.dumps(schema)}\"\n",
    "    return prompt\n",
    "\n",
    "print(\"JSON output handling utility defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this guide, we've covered:\n",
    "\n",
    "1. **Understanding the ChatCompletionClient interface** - The abstract methods you need to implement\n",
    "2. **Creating a basic custom client** - Using HuggingFace Transformers as the backend\n",
    "3. **Implementing required methods** - Both `create()` and `create_stream()`\n",
    "4. **Integration with AutoGen** - How to use your custom client with agents\n",
    "5. **Best practices** - Error handling, configuration, and capability declaration\n",
    "6. **Advanced features** - Function calling and JSON output support\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Custom model clients must implement the `ChatCompletionClient` abstract interface\n",
    "- Use Pydantic models for configuration and type safety\n",
    "- Properly handle async operations to avoid blocking the event loop\n",
    "- Accurately declare model capabilities to ensure proper integration\n",
    "- Implement proper error handling for production use\n",
    "- Consider token usage tracking for cost management\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Adapt this example to use your specific model or API\n",
    "- Implement additional features like function calling or vision support\n",
    "- Add comprehensive error handling and logging\n",
    "- Create tests for your custom model client\n",
    "- Consider packaging your client as a reusable component\n",
    "\n",
    "For more examples and patterns, see the existing model client implementations in the `autogen_ext.models` package."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}