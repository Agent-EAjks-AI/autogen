{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple eval\n",
    "\n",
    "\n",
    "### AgentSpec (may need new name)\n",
    "\n",
    "Wrapper class with a run method to represent any type of function we want to evaluate\n",
    "- BaseAgentSpec\n",
    "  - run() -> TeamResult\n",
    "- AgentChatSpec\n",
    "  - run(agent: Team | Agent, task: str | ....), calls agent.run -> TeamResult\n",
    "  - should track duration and total tokens?\n",
    "\n",
    "\n",
    "\n",
    "### TaskRunner\n",
    "- take a list of tasks (task is str or may have multimodal?)\n",
    "- take a list of team configs \n",
    "- run team configs against tasks (in background offline task)\n",
    "- each EvalRun should produce an EvalResult\n",
    "  - result from TeamManager.run\n",
    "- each run should be backed by DB... we should be able to kick off a run in the UI and see its status. \n",
    "- Each run should have comprehensive metrics that we can show in UI ... start with what team result already provides\n",
    "- should be possible to restart a run (we check db for those not completed and rerun them) \n",
    "- class TaskRunner\n",
    "  - run(agent_spec, task) calls agent_spec.run(task)\n",
    "\n",
    "eval = EvalRunner()\n",
    "task = [\"height of eiffel\", \"depth of lake sevan\"]\n",
    "agent_specs = [agent, ]\n",
    "eval.run() \n",
    "\n",
    "\n",
    "\n",
    "EvalJudge \n",
    "- BaseEvalJudge\n",
    "  - judge() calls eval_func\n",
    "  - eval_func () -> EvalResult (status: bool, reason: str, metadata:Any)\n",
    "    - EvalResult shoud be [{dim: status, value: 0/1, reason:str}]\n",
    "- LLMEvalJudge\n",
    "  - eval_func (criteria, agent_results )\n",
    "    - EvalResut should be\n",
    "      - {dimension: status, value: 0/1, reason:str, max_value:1}\n",
    "      - {dimension: trajectory_efficiency, value: 0/10, reason:str, max_value+10}\n",
    "    - \n",
    "\n",
    "judge = LLMEvalJudge\n",
    "criteria = [{dimension: str, prompt: str, }]\n",
    "results = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am working on creating an evaluation platform for my agent traces ...\n",
    "We should have base classes that allow flexible behaviour but also high level classes that tie to AGS specific abstractions like teams etc \n",
    "\n",
    "I should be able to have sample code that defines a list of tasks (which may be string or list of string and images etc), define a list of team configurations, run them teams against the tasks, and then judge the traces, (). Ideally running tasks should be background optimized (posislby threaded or asyn tasks?) as well as runing judgements. All of these things should also be logged to db with status for each run etc There should be a way to kick off a run in the UI and see its status. Each run should have comprehensive metrics that we can show in UI ... start with what team result already provides. It should be possible to restart a run (we check db for those not completed and rerun them).\n",
    "\n",
    "Dont write any code yet, just reason first and let us come up with a reasonable deisgn. If there is any file you need e.g, a definition of some datamodel, you should let me konw .. do not assume shit that will cuase mypy errors\n",
    "\n",
    "\n",
    " \n",
    "## Task Definition \n",
    "\n",
    "We should have a way to define a task as a sequence of test and images and files as needed. \n",
    "\n",
    "eval.task?\n",
    "\n",
    "task = EvalTask([\"what is the height of the eiffel tower, only respond with the number in meters])\n",
    "task = EvalTask([\"what is in this image\"], Image)\n",
    "\n",
    "## Run Configuration\n",
    "\n",
    "We should have a EvalRun configuration that tells us what should be executed when a task is run. IT should be cusotmmizable, e.g., a could just be that this is given to a model and the result returend. \n",
    "\n",
    "Should take a task \n",
    "\n",
    "\n",
    "BaseEvalRunner\n",
    "   init()\n",
    "   run()\n",
    "\n",
    "LLMEvalRunner\n",
    "   init(model_client) \n",
    "   run(task)\n",
    "   result = TeamResult(...) \n",
    "   return result \n",
    "\n",
    "\n",
    "TeamEvalRunner\n",
    "    run(team_config)\n",
    "    result = TeamResult(...)\n",
    "\n",
    "\n",
    "## Judge\n",
    "\n",
    "THis should take a task result and return some type of structured unified score object. ideally a high level unified score and then granular scores on a set of dimensions.\n",
    "\n",
    "EvalScore\n",
    " - score (int or none)\n",
    " - dimensions\n",
    "   - score (int or none)\n",
    "   - reason (str)\n",
    "   - max_value (int)\n",
    "   - dimension (str)\n",
    "   - metadata (any)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BaseEvalJudge\n",
    "   run(task, result) \n",
    "\n",
    "LLM judge will provide overall screo and  per dimension scores using criteria\n",
    "   - eval_func (criteria, agent_results)\n",
    "   - criteria = [{dimension: str, prompt: str, }]\n",
    "   - results = EvalScore \n",
    "\n",
    "\n",
    "EvalJudgeCriteria\n",
    "   - dimension (str)\n",
    "   - prompt (str) \n",
    "   - max_value (int)\n",
    "   - metadata (any) \n",
    "  \n",
    "LLMEvalJudge\n",
    "   run(task, result, criteria)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "At the end we should have a system that can take a task, run it against a set of teams, and return a score. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " ## Task Runner  \n",
    "\n",
    "\n",
    " ## Judge   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Vars\n",
    "\n",
    "I'd like to create infra to maintiain context vars for runs.\n",
    "- each time a run starts in ws.py or in startstream, we should have a context var that should have things like userid/runid/readable/timetamp. can we have a module that sets this\n",
    "- tools should be able to read this variable ..import and read this variable and use it? e.,g if they will use this to write files to a directory if it exists ... later my app can scan that dir and then use that share info to front end .. e.,g if a tool geneerates a image we can show thqt in front end\n",
    "\n",
    "\n",
    "- Update TeamManager \n",
    "  - create /userid/run/ ....\n",
    "  - to scan for changes in /run/ ... and udpate metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Evaluations ===\n",
      "\n",
      "Running model evaluations...\n",
      "  Evaluating task: Eiffel Tower Height\n",
      "  Model response: messages=[TextMessage(source='model', models_usage=None, metadata={}, content=\"{'finish_reason': 'stop', 'content': 'The Eiffel Tower stands at a height of approximately 1,083 feet (330 meters) including its antennas. The structure itself is about 1,063 feet (324 meters) tall without the antennas.', 'usage': {'prompt_tokens': 17, 'completion_tokens': 42}, 'cached': False, 'logprobs': None, 'thought': None}\", type='TextMessage')] stop_reason=None\n",
      "  Evaluating task: Lake Tanganyika Depth\n",
      "  Model response: messages=[TextMessage(source='model', models_usage=None, metadata={}, content=\"{'finish_reason': 'stop', 'content': 'Lake Tanganyika is one of the deepest lakes in the world, with a maximum depth of approximately 1,470 meters (4,823 feet). It is the second deepest lake in Africa, after Lake Malawi.', 'usage': {'prompt_tokens': 17, 'completion_tokens': 44}, 'cached': False, 'logprobs': None, 'thought': None}\", type='TextMessage')] stop_reason=None\n",
      "\n",
      "Running team evaluations...\n",
      "  Evaluating task: Eiffel Tower Height\n",
      "  Team response: The Eiffel Tower has a total height of approximately 1,083 feet (330 meters) when including its ante...\n",
      "  Evaluating task: Lake Tanganyika Depth\n",
      "  Team response: Lake Tanganyika is one of the deepest lakes in the world, reaching a maximum depth of about 4,823 fe...\n",
      "\n",
      "=== Judging Results ===\n",
      "\n",
      "Judging model results...\n",
      "  Judging task: Eiffel Tower Height\n",
      "  Overall score: 0.0\n",
      "    accuracy: 0.0 - The response provided does not include any factual...\n",
      "    completeness: 0.0 - The response does not provide any information rega...\n",
      "  Judging task: Lake Tanganyika Depth\n",
      "  Overall score: 0.0\n",
      "    accuracy: 0.0 - The provided response does not contain any factual...\n",
      "    completeness: 0.0 - The response does not provide any relevant informa...\n",
      "\n",
      "Judging team results...\n",
      "  Judging task: Eiffel Tower Height\n",
      "  Overall score: 0.0\n",
      "    accuracy: 0.0 - The response does not provide any factual informat...\n",
      "    completeness: 0.0 - The response does not include the actual height of...\n",
      "  Judging task: Lake Tanganyika Depth\n",
      "  Overall score: 0.0\n",
      "    accuracy: 0.0 - The response does not provide the actual depth of ...\n",
      "    completeness: 0.0 - The response does not provide the depth of Lake Ta...\n",
      "\n",
      "=== Testing Serialization and Deserialization ===\n",
      "\n",
      "Serialized model runner: {'provider': 'autogenstudio.eval.runners.ModelEvalRunner', 'component_type': 'eval_runner', 'version': 1, 'component_version': 1, 'description': 'Evaluation runner that uses a single LLM to process tasks.', 'label': 'ModelEvalRunner', 'config': {'name': 'Direct Model Runner', 'description': 'Evaluates tasks by sending them directly to the model', 'metadata': {}, 'model_client': {'provider': 'autogen_ext.models.openai.OpenAIChatCompletionClient', 'component_type': 'model', 'version': 1, 'component_version': 1, 'description': 'Chat completion client for OpenAI hosted models.', 'label': 'OpenAIChatCompletionClient', 'config': {'model': 'gpt-4o-mini'}}}}\n",
      "Deserialized model runner: Direct Model Runner\n",
      "Serialized judge: {'provider': 'autogenstudio.eval.judges.LLMEvalJudge', 'component_type': 'eval_judge', 'version': 1, 'component_version': 1, 'description': 'Judge that uses an LLM to evaluate results.', 'label': 'LLMEvalJudge', 'config': {'name': 'Evaluation Judge', 'description': 'Judges the quality of responses', 'metadata': {}, 'model_client': {'provider': 'autogen_ext.models.openai.OpenAIChatCompletionClient', 'component_type': 'model', 'version': 1, 'component_version': 1, 'description': 'Chat completion client for OpenAI hosted models.', 'label': 'OpenAIChatCompletionClient', 'config': {'model': 'gpt-4o-mini'}}}}\n",
      "Deserialized judge: Evaluation Judge\n",
      "\n",
      "=== Evaluation Results ===\n",
      "\n",
      "Task ID: f647a480-0162-4f2a-b74b-14e8e104b675\n",
      "  Model Result: messages=[TextMessage(source='model', models_usage=None, metadata={}, content=\"{'finish_reason': 'stop', 'content': 'The Eiffel Tower stands at a height of approximately 1,083 feet (330 meters) including its antennas. The structure itself is about 1,063 feet (324 meters) tall without the antennas.', 'usage': {'prompt_tokens': 17, 'completion_tokens': 42}, 'cached': False, 'logprobs': None, 'thought': None}\", type='TextMessage')] stop_reason=None\n",
      "Task ID: 309c797f-7fc2-422e-a664-87f38706cad3\n",
      "  Model Result: messages=[TextMessage(source='model', models_usage=None, metadata={}, content=\"{'finish_reason': 'stop', 'content': 'Lake Tanganyika is one of the deepest lakes in the world, with a maximum depth of approximately 1,470 meters (4,823 feet). It is the second deepest lake in Africa, after Lake Malawi.', 'usage': {'prompt_tokens': 17, 'completion_tokens': 44}, 'cached': False, 'logprobs': None, 'thought': None}\", type='TextMessage')] stop_reason=None\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "# Import necessary components\n",
    "from autogen_core import ComponentModel\n",
    "from autogen_core.models import UserMessage\n",
    "from autogen_agentchat.agents import AssistantAgent \n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Import the evaluation components we created\n",
    "from autogenstudio.datamodel.eval import EvalTask, EvalRunResult, EvalJudgeCriteria, EvalScore, EvalRunStatus\n",
    "from autogenstudio.eval.runners import ModelEvalRunner, TeamEvalRunner\n",
    "from autogenstudio.eval.judges import LLMEvalJudge\n",
    "\n",
    "# This example demonstrates how to run evaluations with different runners and judge the results\n",
    "\n",
    "async def run_simple_evaluation():\n",
    "    \"\"\"Run a simple evaluation of model and team responses.\"\"\"\n",
    "    \n",
    "    # Step 1: Create a model client\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        # api_key is loaded from environment variable OPENAI_API_KEY\n",
    "    )\n",
    "    \n",
    "    # Step 2: Create evaluation tasks\n",
    "    tasks = [\n",
    "        EvalTask( \n",
    "            name=\"Eiffel Tower Height\",\n",
    "            description=\"Answer the question about the Eiffel Tower height\",\n",
    "            input=\"What is the height of the Eiffel Tower?\"\n",
    "        ),\n",
    "        EvalTask( \n",
    "            name=\"Lake Tanganyika Depth\",\n",
    "            description=\"Answer the question about Lake Tanganyika's depth\",\n",
    "            input=\"What is the depth of Lake Tanganyika?\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Step 3: Create evaluation runners\n",
    "    \n",
    "    # 3.1: Model runner (direct model access)\n",
    "    model_runner = ModelEvalRunner(\n",
    "        model_client=model_client,\n",
    "        name=\"Direct Model Runner\",\n",
    "        description=\"Evaluates tasks by sending them directly to the model\"\n",
    "    )\n",
    "    \n",
    "    # 3.2: Team runner (using a simple team with one agent)\n",
    "    # Create an assistant agent for the team\n",
    "    agent = AssistantAgent(\n",
    "        name=\"research_agent\",\n",
    "        model_client=model_client,\n",
    "        system_message=\"You are a helpful assistant\"\n",
    "    )\n",
    "    \n",
    "    # Create a team with the agent\n",
    "    team = RoundRobinGroupChat(participants=[agent], max_turns=3)\n",
    "    \n",
    "    # Create a team runner with the team\n",
    "    team_runner = TeamEvalRunner(\n",
    "        team=team,\n",
    "        name=\"Team Runner\",\n",
    "        description=\"Evaluates tasks using a team of agents\"\n",
    "    )\n",
    "    \n",
    "    # Step 4: Create an LLM judge\n",
    "    # We use the same model client for simplicity\n",
    "    judge = LLMEvalJudge(\n",
    "        model_client=model_client,\n",
    "        name=\"Evaluation Judge\",\n",
    "        description=\"Judges the quality of responses\"\n",
    "    )\n",
    "    \n",
    "    # Step 5: Define evaluation criteria\n",
    "    criteria = [\n",
    "        EvalJudgeCriteria(\n",
    "            dimension=\"accuracy\",\n",
    "            prompt=\"Evaluate the factual accuracy of the response. Are all facts correct?\",\n",
    "            min_value=0,\n",
    "            max_value=10\n",
    "        ),\n",
    "        EvalJudgeCriteria(\n",
    "            dimension=\"completeness\",\n",
    "            prompt=\"Evaluate how thoroughly the response addresses the question. Does it provide all relevant information?\",\n",
    "            min_value=0,\n",
    "            max_value=10\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Step 6: Run evaluations and judge the results\n",
    "    results = {}\n",
    "    \n",
    "    print(\"\\n=== Running Evaluations ===\\n\")\n",
    "    \n",
    "    # Run model evaluations\n",
    "    print(\"Running model evaluations...\")\n",
    "    model_results = {}\n",
    "    for task in tasks:\n",
    "        print(f\"  Evaluating task: {task.name}\")\n",
    "        model_result = await model_runner.run(task)\n",
    "        model_results[task.task_id] = model_result\n",
    "        \n",
    "        # Print model response\n",
    "        if model_result.status:\n",
    "            print(f\"  Model response: {model_result.result}\")\n",
    "        else:\n",
    "            print(f\"  Model error: {model_result.error}\")\n",
    "    \n",
    "    # Run team evaluations\n",
    "    print(\"\\nRunning team evaluations...\")\n",
    "    team_results = {}\n",
    "    for task in tasks:\n",
    "        print(f\"  Evaluating task: {task.name}\")\n",
    "        team_result = await team_runner.run(task)\n",
    "        team_results[task.task_id] = team_result\n",
    "        \n",
    "        # Print team response\n",
    "        if team_result.status:\n",
    "            messages = team_result.result.messages or []\n",
    "            final_message = messages[-1] if messages else None\n",
    "            if final_message and final_message is not None:\n",
    "                print(f\"  Team response: {final_message.content[:100]}...\")\n",
    "            else:\n",
    "                print(\"  No response from team\")\n",
    "        else:\n",
    "            print(f\"  Team error: {team_result.error}\")\n",
    "    \n",
    "    # Judge the results\n",
    "    print(\"\\n=== Judging Results ===\\n\")\n",
    "    \n",
    "    # Judge model results\n",
    "    print(\"Judging model results...\")\n",
    "    model_scores = {}\n",
    "    for task in tasks:\n",
    "        if task.task_id in model_results and model_results[task.task_id].status:\n",
    "            print(f\"  Judging task: {task.name}\")\n",
    "            model_score = await judge.judge(task, model_results[task.task_id], criteria)\n",
    "            model_scores[task.task_id] = model_score\n",
    "            \n",
    "            # Print scores\n",
    "            print(f\"  Overall score: {model_score.overall_score}\")\n",
    "            for dimension_score in model_score.dimension_scores:\n",
    "                print(f\"    {dimension_score.dimension}: {dimension_score.score} - {dimension_score.reason[:50]}...\")\n",
    "    \n",
    "    # Judge team results\n",
    "    print(\"\\nJudging team results...\")\n",
    "    team_scores = {}\n",
    "    for task in tasks:\n",
    "        if task.task_id in team_results and team_results[task.task_id].status:\n",
    "            print(f\"  Judging task: {task.name}\")\n",
    "            team_score = await judge.judge(task, team_results[task.task_id], criteria)\n",
    "            team_scores[task.task_id] = team_score\n",
    "            \n",
    "            # Print scores\n",
    "            print(f\"  Overall score: {team_score.overall_score}\")\n",
    "            for dimension_score in team_score.dimension_scores:\n",
    "                print(f\"    {dimension_score.dimension}: {dimension_score.score} - {dimension_score.reason[:50]}...\")\n",
    "    \n",
    "    # Step 7: Test serialization and deserialization\n",
    "    print(\"\\n=== Testing Serialization and Deserialization ===\\n\")\n",
    "    \n",
    "    # Serialize model runner\n",
    "    model_runner_config = model_runner.dump_component()\n",
    "    print(f\"Serialized model runner: {model_runner_config.model_dump()}\")\n",
    "    \n",
    "    # Deserialize model runner\n",
    "    deserialized_model_runner = ModelEvalRunner.load_component(model_runner_config)\n",
    "    print(f\"Deserialized model runner: {deserialized_model_runner.name}\")\n",
    "    \n",
    "    # Serialize judge\n",
    "    judge_config = judge.dump_component()\n",
    "    print(f\"Serialized judge: {judge_config.model_dump()}\")\n",
    "    \n",
    "    # Deserialize judge\n",
    "    deserialized_judge = LLMEvalJudge.load_component(judge_config)\n",
    "    print(f\"Deserialized judge: {deserialized_judge.name}\")\n",
    "    \n",
    "    # Close the model client\n",
    "    await model_client.close()\n",
    "    \n",
    "    return {\n",
    "        \"model_results\": model_results,\n",
    "        \"team_results\": team_results,\n",
    "        \"model_scores\": model_scores,\n",
    "        \"team_scores\": team_scores\n",
    "    }\n",
    "\n",
    "# For running in a notebook\n",
    "results = await run_simple_evaluation()\n",
    "\n",
    "# Print the results\n",
    "print(\"\\n=== Evaluation Results ===\\n\")\n",
    "for task_id, model_result in results[\"model_results\"].items():\n",
    "    print(f\"Task ID: {task_id}\")\n",
    "    print(f\"  Model Result: {model_result.result if model_result.status else model_result.error}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-05 14:44:20.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mautogenstudio.eval.orchestrator\u001b[0m:\u001b[36m_execute_run\u001b[0m:\u001b[36m354\u001b[0m - \u001b[1mStarting runner for run a48f657e-5057-4f83-a711-3a5f88c5ced5\u001b[0m\n",
      "\u001b[32m2025-04-05 14:44:21.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mautogenstudio.eval.orchestrator\u001b[0m:\u001b[36m_execute_run\u001b[0m:\u001b[36m354\u001b[0m - \u001b[1mStarting runner for run 2c977e89-2e15-4176-9f89-5666b3cf5959\u001b[0m\n",
      "\u001b[32m2025-04-05 14:44:21.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mautogenstudio.eval.orchestrator\u001b[0m:\u001b[36m_execute_run\u001b[0m:\u001b[36m354\u001b[0m - \u001b[1mStarting runner for run 09584e22-4a6d-4663-8ff6-0da3d70226d1\u001b[0m\n",
      "\u001b[32m2025-04-05 14:44:21.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mautogenstudio.eval.orchestrator\u001b[0m:\u001b[36m_execute_run\u001b[0m:\u001b[36m354\u001b[0m - \u001b[1mStarting runner for run 7b4bee41-3fb4-4e3c-babc-9f56e4b42b0f\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Evaluation Runs ===\n",
      "\n",
      "Starting model runs...\n",
      "Starting team runs...\n",
      "\n",
      "=== Waiting for Runs to Complete ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-05 14:44:21.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mautogenstudio.eval.orchestrator\u001b[0m:\u001b[36m_execute_run\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1mStarting judge for run a48f657e-5057-4f83-a711-3a5f88c5ced5\u001b[0m\n",
      "\u001b[32m2025-04-05 14:44:21.899\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mautogenstudio.eval.orchestrator\u001b[0m:\u001b[36m_execute_run\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1mStarting judge for run 2c977e89-2e15-4176-9f89-5666b3cf5959\u001b[0m\n",
      "\u001b[32m2025-04-05 14:44:24.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mautogenstudio.eval.orchestrator\u001b[0m:\u001b[36m_execute_run\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1mStarting judge for run 7b4bee41-3fb4-4e3c-babc-9f56e4b42b0f\u001b[0m\n",
      "\u001b[32m2025-04-05 14:44:24.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mautogenstudio.eval.orchestrator\u001b[0m:\u001b[36m_execute_run\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mRun a48f657e-5057-4f83-a711-3a5f88c5ced5 completed successfully\u001b[0m\n",
      "\u001b[32m2025-04-05 14:44:24.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mautogenstudio.eval.orchestrator\u001b[0m:\u001b[36m_execute_run\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1mStarting judge for run 09584e22-4a6d-4663-8ff6-0da3d70226d1\u001b[0m\n",
      "\u001b[32m2025-04-05 14:44:24.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mautogenstudio.eval.orchestrator\u001b[0m:\u001b[36m_execute_run\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mRun 2c977e89-2e15-4176-9f89-5666b3cf5959 completed successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run a48f657e-5057-4f83-a711-3a5f88c5ced5 completed with status: EvalRunStatus.COMPLETED\n",
      "Run 2c977e89-2e15-4176-9f89-5666b3cf5959 completed with status: EvalRunStatus.COMPLETED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-05 14:44:27.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mautogenstudio.eval.orchestrator\u001b[0m:\u001b[36m_execute_run\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mRun 7b4bee41-3fb4-4e3c-babc-9f56e4b42b0f completed successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 7b4bee41-3fb4-4e3c-babc-9f56e4b42b0f completed with status: EvalRunStatus.COMPLETED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-05 14:44:28.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mautogenstudio.eval.orchestrator\u001b[0m:\u001b[36m_execute_run\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mRun 09584e22-4a6d-4663-8ff6-0da3d70226d1 completed successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 09584e22-4a6d-4663-8ff6-0da3d70226d1 completed with status: EvalRunStatus.COMPLETED\n",
      "\n",
      "=== Evaluation Results ===\n",
      "\n",
      "Model run results:\n",
      "Run ID: a48f657e-5057-4f83-a711-3a5f88c5ced5\n",
      "  Response: TaskResult(messages=[TextMessage(content=\"{'finish_reason': 'stop', 'content': 'The Eiffel Tower stands at approximately 1,083 feet (330 meters) tall, including its antennas. Without the antennas, its height is about 1,063 feet (324 meters).', 'usage': {'prompt_tokens': 17, 'completion_tokens': 39}, 'cached': False, 'logprobs': None, 'thought': None}\", source='model', models_usage=None, metadata={}, type='TextMessage')], stop_reason=None)\n",
      "  Overall score: 9.25\n",
      "    accuracy: 10.0\n",
      "    Reason: The response correctly states that the Eiffel Tower stands at approximately 1,083 feet (330 meters) ...\n",
      "    completeness: 8.5\n",
      "    Reason: The response provides a clear and accurate height of the Eiffel Tower with two measurements: approxi...\n",
      "Run ID: 2c977e89-2e15-4176-9f89-5666b3cf5959\n",
      "  Response: TaskResult(messages=[TextMessage(content=\"{'finish_reason': 'stop', 'content': 'Lake Tanganyika is one of the deepest lakes in the world. Its maximum depth is approximately 1,470 meters (4,823 feet). It is the second deepest lake after Lake Baikal in Russia.', 'usage': {'prompt_tokens': 17, 'completion_tokens': 43}, 'cached': False, 'logprobs': None, 'thought': None}\", source='model', models_usage=None, metadata={}, type='TextMessage')], stop_reason=None)\n",
      "  Overall score: 9.5\n",
      "    accuracy: 10.0\n",
      "    Reason: The response accurately states that Lake Tanganyika has a maximum depth of approximately 1,470 meter...\n",
      "    completeness: 9.0\n",
      "    Reason: The response thoroughly addresses the question about Lake Tanganyika's depth by stating its maximum ...\n",
      "\n",
      "Team run results:\n",
      "Run ID: 09584e22-4a6d-4663-8ff6-0da3d70226d1\n",
      "  Response: The Eiffel Tower stands at a height of approximately 300 meters (1,083 feet), including its antennas...\n",
      "  Overall score: 8.5\n",
      "    accuracy: 9.0\n",
      "    Reason: The response provided accurate information regarding the height of the Eiffel Tower, noting it stand...\n",
      "    completeness: 8.0\n",
      "    Reason: The response provides a thorough answer to the question about the height of the Eiffel Tower, mentio...\n",
      "Run ID: 7b4bee41-3fb4-4e3c-babc-9f56e4b42b0f\n",
      "  Response: Lake Tanganyika is one of the deepest lakes in the world, with a maximum depth of about 1,470 meters...\n",
      "  Overall score: 9.5\n",
      "    accuracy: 10.0\n",
      "    Reason: The response accurately provides the depth of Lake Tanganyika as approximately 1,470 meters (4,823 f...\n",
      "    completeness: 9.0\n",
      "    Reason: The response effectively answers the question on Lake Tanganyika's depth by providing the maximum de...\n",
      "\n",
      "=== Orchestrated Evaluation Results ===\n",
      "\n",
      "Task ID: 0f12705d-af15-40f3-ba61-aab4035095d9, Model Run ID: a48f657e-5057-4f83-a711-3a5f88c5ced5\n",
      "Task ID: 64cdbf50-c92a-4d18-94b0-d2c745b6c2c0, Model Run ID: 2c977e89-2e15-4176-9f89-5666b3cf5959\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "# Import necessary components\n",
    "from autogen_core import ComponentModel\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent \n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "\n",
    "# Import the evaluation components\n",
    "from autogenstudio.datamodel.eval import EvalTask, EvalJudgeCriteria, EvalRunStatus\n",
    "from autogenstudio.eval.runners import ModelEvalRunner, TeamEvalRunner\n",
    "from autogenstudio.eval.judges import LLMEvalJudge\n",
    "from autogenstudio.eval.orchestrator import EvalOrchestrator  # New import\n",
    "\n",
    "async def run_orchestrated_evaluation():\n",
    "    \"\"\"Run a simple evaluation using the EvalOrchestrator.\"\"\"\n",
    "    \n",
    "    # Step 1: Create a model client\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        # api_key is loaded from environment variable OPENAI_API_KEY\n",
    "    )\n",
    "    \n",
    "    # Step 2: Create an orchestrator (without DB for this example)\n",
    "    orchestrator = EvalOrchestrator()\n",
    "    \n",
    "    # Step 3: Create and register tasks\n",
    "    task_ids = []\n",
    "    tasks = [\n",
    "        EvalTask( \n",
    "            name=\"Eiffel Tower Height\",\n",
    "            description=\"Answer the question about the Eiffel Tower height\",\n",
    "            input=\"What is the height of the Eiffel Tower?\"\n",
    "        ),\n",
    "        EvalTask( \n",
    "            name=\"Lake Tanganyika Depth\",\n",
    "            description=\"Answer the question about Lake Tanganyika's depth\",\n",
    "            input=\"What is the depth of Lake Tanganyika?\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    for task in tasks:\n",
    "        task_id = await orchestrator.create_task(task)\n",
    "        task_ids.append(task_id)\n",
    "    \n",
    "    # Step 4: Create and register criteria\n",
    "    criteria_ids = []\n",
    "    criteria = [\n",
    "        EvalJudgeCriteria(\n",
    "            dimension=\"accuracy\",\n",
    "            prompt=\"Evaluate the factual accuracy of the response. Are all facts correct?\",\n",
    "            min_value=0,\n",
    "            max_value=10\n",
    "        ),\n",
    "        EvalJudgeCriteria(\n",
    "            dimension=\"completeness\",\n",
    "            prompt=\"Evaluate how thoroughly the response addresses the question. Does it provide all relevant information?\",\n",
    "            min_value=0,\n",
    "            max_value=10\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    for criterion in criteria:\n",
    "        criterion_id = await orchestrator.create_criteria(criterion)\n",
    "        criteria_ids.append(criterion_id)\n",
    "    \n",
    "    # Step 5: Create runners\n",
    "    # Model runner\n",
    "    model_runner = ModelEvalRunner(\n",
    "        model_client=model_client,\n",
    "        name=\"Direct Model Runner\",\n",
    "        description=\"Evaluates tasks by sending them directly to the model\"\n",
    "    )\n",
    "    \n",
    "    # Team runner\n",
    "    agent = AssistantAgent(\n",
    "        name=\"research_agent\",\n",
    "        model_client=model_client,\n",
    "        system_message=\"You are a helpful assistant\"\n",
    "    )\n",
    "    team = RoundRobinGroupChat(participants=[agent], max_turns=3)\n",
    "    team_runner = TeamEvalRunner(\n",
    "        team=team,\n",
    "        name=\"Team Runner\",\n",
    "        description=\"Evaluates tasks using a team of agents\"\n",
    "    )\n",
    "    \n",
    "    # Step 6: Create a judge\n",
    "    judge = LLMEvalJudge(\n",
    "        model_client=model_client,\n",
    "        name=\"Evaluation Judge\",\n",
    "        description=\"Judges the quality of responses\"\n",
    "    )\n",
    "    \n",
    "    # Step 7: Create evaluation runs\n",
    "    model_run_ids = []\n",
    "    team_run_ids = []\n",
    "    \n",
    "    # Create model runs\n",
    "    for task_id in task_ids:\n",
    "        run_id = await orchestrator.create_run(\n",
    "            task=task_id,\n",
    "            runner=model_runner,\n",
    "            judge=judge,\n",
    "            criteria=criteria_ids,\n",
    "            name=f\"Model Run - Task {task_id}\"\n",
    "        )\n",
    "        model_run_ids.append(run_id)\n",
    "    \n",
    "    # Create team runs\n",
    "    for task_id in task_ids:\n",
    "        run_id = await orchestrator.create_run(\n",
    "            task=task_id,\n",
    "            runner=team_runner,\n",
    "            judge=judge,\n",
    "            criteria=criteria_ids,\n",
    "            name=f\"Team Run - Task {task_id}\"\n",
    "        )\n",
    "        team_run_ids.append(run_id)\n",
    "    \n",
    "    # Step 8: Execute the runs\n",
    "    print(\"\\n=== Starting Evaluation Runs ===\\n\")\n",
    "    \n",
    "    # Start model runs\n",
    "    print(\"Starting model runs...\")\n",
    "    for run_id in model_run_ids:\n",
    "        await orchestrator.start_run(run_id)\n",
    "    \n",
    "    # Start team runs\n",
    "    print(\"Starting team runs...\")\n",
    "    for run_id in team_run_ids:\n",
    "        await orchestrator.start_run(run_id)\n",
    "    \n",
    "    # Step 9: Wait for runs to complete\n",
    "    print(\"\\n=== Waiting for Runs to Complete ===\\n\")\n",
    "    \n",
    "    all_runs = model_run_ids + team_run_ids\n",
    "    completed = {run_id: False for run_id in all_runs}\n",
    "    \n",
    "    while not all(completed.values()):\n",
    "        for run_id in all_runs:\n",
    "            if not completed[run_id]:\n",
    "                status = await orchestrator.get_run_status(run_id)\n",
    "                if status in [EvalRunStatus.COMPLETED, EvalRunStatus.FAILED, EvalRunStatus.CANCELED]:\n",
    "                    completed[run_id] = True\n",
    "                    print(f\"Run {run_id} completed with status: {status}\")\n",
    "        \n",
    "        await asyncio.sleep(1)\n",
    "    \n",
    "    # Step 10: Get results\n",
    "    print(\"\\n=== Evaluation Results ===\\n\")\n",
    "    \n",
    "    # Model results\n",
    "    print(\"Model run results:\")\n",
    "    for run_id in model_run_ids:\n",
    "        run_result = await orchestrator.get_run_result(run_id)\n",
    "        score_result = await orchestrator.get_run_score(run_id)\n",
    "        \n",
    "        print(f\"Run ID: {run_id}\")\n",
    "        if run_result and run_result.status:\n",
    "            print(f\"  Response: {run_result.result}\")\n",
    "            \n",
    "            if score_result:\n",
    "                print(f\"  Overall score: {score_result.overall_score}\")\n",
    "                for dimension_score in score_result.dimension_scores:\n",
    "                    print(f\"    {dimension_score.dimension}: {dimension_score.score}\")\n",
    "                    print(f\"    Reason: {dimension_score.reason[:100]}...\")\n",
    "        else:\n",
    "            print(f\"  Error: {run_result.error if run_result else 'No result'}\")\n",
    "    \n",
    "    # Team results\n",
    "    print(\"\\nTeam run results:\")\n",
    "    for run_id in team_run_ids:\n",
    "        run_result = await orchestrator.get_run_result(run_id)\n",
    "        score_result = await orchestrator.get_run_score(run_id)\n",
    "        \n",
    "        print(f\"Run ID: {run_id}\")\n",
    "        if run_result and run_result.status:\n",
    "            messages = run_result.result.messages or []\n",
    "            final_message = messages[-1] if messages else None\n",
    "            if final_message and final_message is not None:\n",
    "                print(f\"  Response: {final_message.content[:100]}...\")\n",
    "            \n",
    "            if score_result:\n",
    "                print(f\"  Overall score: {score_result.overall_score}\")\n",
    "                for dimension_score in score_result.dimension_scores:\n",
    "                    print(f\"    {dimension_score.dimension}: {dimension_score.score}\")\n",
    "                    print(f\"    Reason: {dimension_score.reason[:100]}...\")\n",
    "        else:\n",
    "            print(f\"  Error: {run_result.error if run_result else 'No result'}\")\n",
    "    \n",
    "    # Close the model client\n",
    "    await model_client.close()\n",
    "    \n",
    "    return {\n",
    "        \"task_ids\": task_ids,\n",
    "        \"criteria_ids\": criteria_ids,\n",
    "        \"model_run_ids\": model_run_ids,\n",
    "        \"team_run_ids\": team_run_ids\n",
    "    }\n",
    "\n",
    "# For running in a notebook\n",
    "results = await run_orchestrated_evaluation()\n",
    "print(\"\\n=== Orchestrated Evaluation Results ===\\n\")\n",
    "for task_id, run_id in zip(results[\"task_ids\"], results[\"model_run_ids\"]):\n",
    "    print(f\"Task ID: {task_id}, Model Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_ids': ['0f12705d-af15-40f3-ba61-aab4035095d9',\n",
       "  '64cdbf50-c92a-4d18-94b0-d2c745b6c2c0'],\n",
       " 'criteria_ids': ['21c11ae1-f481-4bc3-a358-d4597cda6a1c',\n",
       "  'b9ed83c3-9bd1-4bf4-8ae0-acffb8310c32'],\n",
       " 'model_run_ids': ['a48f657e-5057-4f83-a711-3a5f88c5ced5',\n",
       "  '2c977e89-2e15-4176-9f89-5666b3cf5959'],\n",
       " 'team_run_ids': ['09584e22-4a6d-4663-8ff6-0da3d70226d1',\n",
       "  '7b4bee41-3fb4-4e3c-babc-9f56e4b42b0f']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient, OpenAIChatCompletionClient\n",
    "from autogen_ext.models.anthropic import AnthropicChatCompletionClient\n",
    "from autogen_core.models import ModelInfo\n",
    "\n",
    "model_client=OpenAIChatCompletionClient(\n",
    "            model=\"gpt-4o-mini\",\n",
    "        )\n",
    "print(model_client.dump_component().model_dump_json())\n",
    "\n",
    "\n",
    "az_model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=\"{your-azure-deployment}\",\n",
    "    model=\"gpt-4o\",\n",
    "    api_version=\"2024-06-01\",\n",
    "    azure_endpoint=\"https://{your-custom-endpoint}.openai.azure.com/\",\n",
    "    api_key=\"sk-...\",\n",
    ")\n",
    "print(az_model_client.dump_component().model_dump_json())\n",
    "\n",
    "anthropic_client = AnthropicChatCompletionClient(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        api_key=\"your-api-key\",  # Optional if ANTHROPIC_API_KEY is set in environment\n",
    "    )\n",
    "print(anthropic_client.dump_component().model_dump_json())\n",
    "\n",
    "mistral_vllm_model = OpenAIChatCompletionClient(\n",
    "        model=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n",
    "        base_url=\"http://localhost:1234/v1\",\n",
    "        model_info=ModelInfo(vision=False, function_calling=True, json_output=False, family=\"unknown\", structured_output=True),\n",
    "    )\n",
    "print(mistral_vllm_model.dump_component().model_dump_json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='The capital of France is Paris.\\n' usage=RequestUsage(prompt_tokens=7, completion_tokens=8) cached=False logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_core.models import ModelInfo\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    model_info=ModelInfo(vision=True, function_calling=True, json_output=True, family=\"unknown\", structured_output=True)\n",
    "    # api_key=\"GEMINI_API_KEY\",\n",
    ")\n",
    "\n",
    "response = await model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n",
    "print(response)\n",
    "await model_client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='{\"thoughts\":\"The user is expressing a positive emotion related to flowers, which can symbolize beauty and happiness. It\\'s a good idea to explore this topic further, perhaps by asking about their favorite flowers or memories associated with them.\",\"response\":\"happy\"}' usage=RequestUsage(prompt_tokens=82, completion_tokens=50) cached=False logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    " \n",
    "\n",
    "# The response format for the agent as a Pydantic base model.\n",
    "class AgentResponse(BaseModel):\n",
    "    thoughts: str\n",
    "    response: Literal[\"happy\", \"sad\", \"neutral\"]\n",
    "\n",
    "\n",
    "# Create an agent that uses the OpenAI GPT-4o model with the custom response format.\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    response_format=AgentResponse,  # type: ignore\n",
    ")\n",
    "\n",
    "# Send a message list to the model and await the response.\n",
    "messages = [\n",
    "    UserMessage(content=\"I am happy today thinking of flowers.  Format as structured json\", source=\"user\"),\n",
    "]\n",
    "response = await model_client.create(messages=messages)\n",
    "assert isinstance(response.content, str)\n",
    "print(response)\n",
    "\n",
    "# parsed_response = AgentResponse.model_validate_json(response.content)\n",
    "# print(parsed_response.thoughts)\n",
    "# print(parsed_response.response)\n",
    "\n",
    "# # Close the connection to the model client.\n",
    "# await model_client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Analyze incident ICM12345 and provide diagnosis.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- diagnosis_advisor ----------\n",
      "[FunctionCall(id='call_f97OxytTQCrFVJGu9pomJRXR', arguments='{\"incident\": \"ICM12345\"}', name='get_icm_info'), FunctionCall(id='call_T46ZgNs5pwtVEu9p8uxZpclk', arguments='{\"svcdata\": \"{\\\\\"incidentId\\\\\":\\\\\"ICM12345\\\\\"}\"}', name='get_svcdeep_diagnosis')]\n",
      "---------- diagnosis_advisor ----------\n",
      "[FunctionExecutionResult(content=\"{'success': True, 'incident_id': 'ICM12345', 'status': 'Active', 'severity': '2'}\", name='get_icm_info', call_id='call_f97OxytTQCrFVJGu9pomJRXR', is_error=False), FunctionExecutionResult(content='{\\'success\\': False, \\'error\\': \"Invalid URL \\'None\\': No scheme supplied. Perhaps you meant https://None?\"}', name='get_svcdeep_diagnosis', call_id='call_T46ZgNs5pwtVEu9p8uxZpclk', is_error=False)]\n",
      "---------- diagnosis_advisor ----------\n",
      "For incident ICM12345, here is the basic information retrieved:\n",
      "\n",
      "- **Incident ID**: ICM12345\n",
      "- **Status**: Active\n",
      "- **Severity**: 2\n",
      "\n",
      "Unfortunately, there was an error when attempting to perform a more detailed diagnostic analysis, likely due to an issue with retrieving specific data or a configuration error regarding the service request.\n",
      "\n",
      "Based on the information available, since the incident is active and has a severity level of '2', it would be important to address it promptly, as severity level 2 usually signifies a high degree of urgency but not critical (which would be severity 1). I recommend confirming the nature of the issue directly through other monitoring or logging systems available to you, as the automated diagnostic tool did not succeed in providing additional insights.\n",
      "\n",
      "If specific symptoms or error messages are associated with ICM12345, they could offer further clues toward the root cause. Let me know if there are any other avenues or specific symptoms you'd like assistance analyzing.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import asyncio\n",
    "from typing import Dict, Any, Optional, Union, List\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "\n",
    "async def get_svcdeep_diagnosis(svcdata: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get service deep diagnosis for an incident.\n",
    "    \n",
    "    Args:\n",
    "        svcdata: Incident data and scenario name in JSON format\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with diagnosis results or error information\n",
    "    \"\"\"\n",
    "    base_url: Optional[str] = os.getenv(\"SVCDEEP_DIAGNOSIS_URL\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            base_url, \n",
    "            json=svcdata)\n",
    "\n",
    "        response.raise_for_status()\n",
    "        data: Dict[str, Any] = response.json()\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"location\": data[\"location\"][\"name\"],\n",
    "            \"temperature_c\": data[\"current\"][\"temp_c\"],\n",
    "            \"condition\": data[\"current\"][\"condition\"][\"text\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "async def get_icm_info(incident: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get ICM information for an incident.\n",
    "    \n",
    "    Args:\n",
    "        incident: Incident identifier or data\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with ICM information or error details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Implementation of ICM info retrieval logic would go here\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"incident_id\": incident,\n",
    "            \"status\": \"Active\",\n",
    "            \"severity\": \"2\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to run the diagnosis agent.\n",
    "    \"\"\"\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "    \n",
    "    diagnosis_agent = AssistantAgent(\n",
    "        name=\"diagnosis_advisor\",\n",
    "        model_client=model_client,\n",
    "        tools=[get_svcdeep_diagnosis, get_icm_info],\n",
    "        system_message=\"You are a diagnosis advisor who can provide possible causation insights and information.\", \n",
    "        model_client_stream=True,\n",
    "    )\n",
    "    icm_analysis = AssistantAgent(\n",
    "        name=\"diagnosis_advisor\",\n",
    "        model_client=model_client,\n",
    "        tools=[get_svcdeep_diagnosis, get_icm_info],\n",
    "        system_message=\"You are a diagnosis advisor who can provide possible causation insights and information.\",\n",
    "        reflect_on_tool_use=True,\n",
    "        model_client_stream=True,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        await Console(agent.run_stream(task=\"Analyze incident ICM12345 and provide diagnosis.\"))\n",
    "    finally:\n",
    "        # Ensure connection is closed even if an error occurs\n",
    "        await model_client.close()\n",
    "\n",
    "# if runninng in a python script\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools: ['browser_close', 'browser_wait', 'browser_resize', 'browser_file_upload', 'browser_install', 'browser_press_key', 'browser_navigate', 'browser_navigate_back', 'browser_navigate_forward', 'browser_pdf_save', 'browser_snapshot', 'browser_click', 'browser_drag', 'browser_hover', 'browser_type', 'browser_select_option', 'browser_take_screenshot', 'browser_tab_list', 'browser_tab_new', 'browser_tab_select', 'browser_tab_close']\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_ext.tools.mcp import StdioServerParams, create_mcp_server_session, mcp_server_tools\n",
    "\n",
    " \n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", parallel_tool_calls=False)  # type: ignore\n",
    "params = StdioServerParams(\n",
    "    command=\"npx\",\n",
    "    args=[\"@playwright/mcp@latest\"],\n",
    "    read_timeout_seconds=60,\n",
    ")\n",
    "async with create_mcp_server_session(params) as session:\n",
    "    await session.initialize()\n",
    "    tools = await mcp_server_tools(server_params=params, session=session)\n",
    "    print(f\"Tools: {[tool.name for tool in tools]}\")\n",
    "\n",
    "    agent = AssistantAgent(\n",
    "        name=\"Assistant\",\n",
    "        model_client=model_client,\n",
    "        tools=tools,  # type: ignore\n",
    "    )\n",
    "\n",
    "    termination = TextMentionTermination(\"TERMINATE\")\n",
    "    team = RoundRobinGroupChat([agent], termination_condition=termination)\n",
    "    # await Console(\n",
    "    #     team.run_stream(\n",
    "    #         task=\"Go to https://ekzhu.com/, visit the first link in the page, then tell me about the linked page.\"\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # await model_client.close() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComponentModel(provider='autogen_agentchat.teams.RoundRobinGroupChat', component_type='team', version=1, component_version=1, description='A team that runs a group chat with participants taking turns in a round-robin fashion\\n    to publish a message to all.', label='RoundRobinGroupChat', config={'participants': [{'provider': 'autogen_agentchat.agents.AssistantAgent', 'component_type': 'agent', 'version': 1, 'component_version': 1, 'description': 'An agent that provides assistance with tool use.', 'label': 'AssistantAgent', 'config': {'name': 'Assistant', 'model_client': {'provider': 'autogen_ext.models.openai.OpenAIChatCompletionClient', 'component_type': 'model', 'version': 1, 'component_version': 1, 'description': 'Chat completion client for OpenAI hosted models.', 'label': 'OpenAIChatCompletionClient', 'config': {'model': 'gpt-4o'}}, 'tools': [{'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_close', 'description': 'Close the page', 'inputSchema': {'type': 'object', 'properties': {}, 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_wait', 'description': 'Wait for a specified time in seconds', 'inputSchema': {'type': 'object', 'properties': {'time': {'type': 'number', 'description': 'The time to wait in seconds'}}, 'required': ['time'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_resize', 'description': 'Resize the browser window', 'inputSchema': {'type': 'object', 'properties': {'width': {'type': 'number', 'description': 'Width of the browser window'}, 'height': {'type': 'number', 'description': 'Height of the browser window'}}, 'required': ['width', 'height'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_file_upload', 'description': 'Upload one or multiple files', 'inputSchema': {'type': 'object', 'properties': {'paths': {'type': 'array', 'items': {'type': 'string'}, 'description': 'The absolute paths to the files to upload. Can be a single file or multiple files.'}}, 'required': ['paths'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_install', 'description': 'Install the browser specified in the config. Call this if you get an error about the browser not being installed.', 'inputSchema': {'type': 'object', 'properties': {}, 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_press_key', 'description': 'Press a key on the keyboard', 'inputSchema': {'type': 'object', 'properties': {'key': {'type': 'string', 'description': 'Name of the key to press or a character to generate, such as `ArrowLeft` or `a`'}}, 'required': ['key'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_navigate', 'description': 'Navigate to a URL', 'inputSchema': {'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'The URL to navigate to'}}, 'required': ['url'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_navigate_back', 'description': 'Go back to the previous page', 'inputSchema': {'type': 'object', 'properties': {}, 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_navigate_forward', 'description': 'Go forward to the next page', 'inputSchema': {'type': 'object', 'properties': {}, 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_pdf_save', 'description': 'Save page as PDF', 'inputSchema': {'type': 'object', 'properties': {}, 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_snapshot', 'description': 'Capture accessibility snapshot of the current page, this is better than screenshot', 'inputSchema': {'type': 'object', 'properties': {}, 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_click', 'description': 'Perform click on a web page', 'inputSchema': {'type': 'object', 'properties': {'element': {'type': 'string', 'description': 'Human-readable element description used to obtain permission to interact with the element'}, 'ref': {'type': 'string', 'description': 'Exact target element reference from the page snapshot'}}, 'required': ['element', 'ref'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_drag', 'description': 'Perform drag and drop between two elements', 'inputSchema': {'type': 'object', 'properties': {'startElement': {'type': 'string', 'description': 'Human-readable source element description used to obtain the permission to interact with the element'}, 'startRef': {'type': 'string', 'description': 'Exact source element reference from the page snapshot'}, 'endElement': {'type': 'string', 'description': 'Human-readable target element description used to obtain the permission to interact with the element'}, 'endRef': {'type': 'string', 'description': 'Exact target element reference from the page snapshot'}}, 'required': ['startElement', 'startRef', 'endElement', 'endRef'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_hover', 'description': 'Hover over element on page', 'inputSchema': {'type': 'object', 'properties': {'element': {'type': 'string', 'description': 'Human-readable element description used to obtain permission to interact with the element'}, 'ref': {'type': 'string', 'description': 'Exact target element reference from the page snapshot'}}, 'required': ['element', 'ref'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_type', 'description': 'Type text into editable element', 'inputSchema': {'type': 'object', 'properties': {'element': {'type': 'string', 'description': 'Human-readable element description used to obtain permission to interact with the element'}, 'ref': {'type': 'string', 'description': 'Exact target element reference from the page snapshot'}, 'text': {'type': 'string', 'description': 'Text to type into the element'}, 'submit': {'type': 'boolean', 'description': 'Whether to submit entered text (press Enter after)'}, 'slowly': {'type': 'boolean', 'description': 'Whether to type one character at a time. Useful for triggering key handlers in the page. By default entire text is filled in at once.'}}, 'required': ['element', 'ref', 'text'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_select_option', 'description': 'Select an option in a dropdown', 'inputSchema': {'type': 'object', 'properties': {'element': {'type': 'string', 'description': 'Human-readable element description used to obtain permission to interact with the element'}, 'ref': {'type': 'string', 'description': 'Exact target element reference from the page snapshot'}, 'values': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Array of values to select in the dropdown. This can be a single value or multiple values.'}}, 'required': ['element', 'ref', 'values'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_take_screenshot', 'description': \"Take a screenshot of the current page. You can't perform actions based on the screenshot, use browser_snapshot for actions.\", 'inputSchema': {'type': 'object', 'properties': {'raw': {'type': 'boolean', 'description': 'Whether to return without compression (in PNG format). Default is false, which returns a JPEG image.'}}, 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_tab_list', 'description': 'List browser tabs', 'inputSchema': {'type': 'object', 'properties': {}, 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_tab_new', 'description': 'Open a new tab', 'inputSchema': {'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'The URL to navigate to in the new tab. If not provided, the new tab will be blank.'}}, 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_tab_select', 'description': 'Select a tab by index', 'inputSchema': {'type': 'object', 'properties': {'index': {'type': 'number', 'description': 'The index of the tab to select'}}, 'required': ['index'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}, {'provider': 'autogen_ext.tools.mcp.StdioMcpToolAdapter', 'component_type': 'tool', 'version': 1, 'component_version': 1, 'description': 'Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.', 'label': 'StdioMcpToolAdapter', 'config': {'server_params': {'command': 'npx', 'args': ['@playwright/mcp@latest'], 'encoding': 'utf-8', 'encoding_error_handler': 'strict', 'read_timeout_seconds': 60.0}, 'tool': {'name': 'browser_tab_close', 'description': 'Close a tab', 'inputSchema': {'type': 'object', 'properties': {'index': {'type': 'number', 'description': 'The index of the tab to close. Closes current tab if not provided.'}}, 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}}}}], 'model_context': {'provider': 'autogen_core.model_context.UnboundedChatCompletionContext', 'component_type': 'chat_completion_context', 'version': 1, 'component_version': 1, 'description': 'An unbounded chat completion context that keeps a view of the all the messages.', 'label': 'UnboundedChatCompletionContext', 'config': {}}, 'description': 'An agent that provides assistance with ability to use tools.', 'system_message': 'You are a helpful AI assistant. Solve tasks using your tools. Reply with TERMINATE when the task has been completed.', 'model_client_stream': False, 'reflect_on_tool_use': False, 'tool_call_summary_format': '{result}', 'metadata': {}}}], 'termination_condition': {'provider': 'autogen_agentchat.conditions.TextMentionTermination', 'component_type': 'termination', 'version': 1, 'component_version': 1, 'description': 'Terminate the conversation if a specific text is mentioned.', 'label': 'TextMentionTermination', 'config': {'text': 'TERMINATE'}}, 'emit_team_events': False})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team.dump_component()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
